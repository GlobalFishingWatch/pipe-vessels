# Vessels pipeline

This repository contains the vessels pipeline, a pipeline which extracts and compiles detailed vessel information.

# Running

## Dependencies

You just need [docker](https://www.docker.com/) and [docker-compose](https://docs.docker.com/compose/) in your machine to run the pipeline. No other dependency is required.

## Setup

The pipeline reads it's input from BigQuery, so you need to first authenticate with your google cloud account inside the docker images. To do that, you need to run this command and follow the instructions:

```
docker-compose run gcloud auth login
```

## Configuration

The pipeline exposes the following standard settings:

* `pipe_vessels.docker_run`: Command to run docker inside the airflow server.
* `pipe_vessels.project_id`: Google cloud project id containing all the resources that running this pipeline requires.
* `pipe_vessels.temp_bucket`: GCS bucket where temp files may be stored to.
* `pipe_vessels.pipeline_bucket`: GCS bucket where all final files generated by this pipeline may be stored to.
* `pipe_vessels.pipeline_dataset`: BigQuery dataset containing various output tables used in this pipeline.
* `pipe_vessels.temp_dataset`: BigQuery dataset containing temporary tables. Should be created with a very short ttl value.
* `pipe_vessels.elasticsearch_server_url`: ElasticSearch server to push vessel information to.
* `pipe_vessels.elasticsearch_server_auth`: ElasticSearch authentication information for the server. Should be in `user:password` format.
* `pipe_vessels.postgres_instance`: CloudSQL postgres instance where the tracks are published to.
* `pipe_vessels.postgres_connection_string`: Connection string for the postgres database to publish the tracks to.

* `pipe_vessels.configurations` an array of configurations for the multiple pipe-vessles dags to run:

* `configuration.name`: The name that will be used to create the dynamic dat. 
* `configuration.check_source_existance`: Defaults to True. Use under your own risk. If the task for checking the source tables will be created or not.
* `configuration.bigquery_vessel_tracks_jinja_query`: BigQuery standardSQL query to generate the information to export to the ElasticSearch index, to see more documentation
about the query, please reffer to the /docs/vessel-tracks/
* `configuration.bigquery_tracks`: BigQuery table to write aggregated tracks into. Defaults to `tracks`.
* `configuration.bigquery_vessel_info_query`: BigQuery standardSQL query to generate the information to export to the ElasticSearch index. The names of the output fields of the query will be the same names that will be available in the ElasticSearch documents. It should contain a `vesselId` field that will be used as the document id.
* `configuration.postgres_table_tracks`: Table in postgres to publish the tracks to.
* `configuration.elasticsearch_index_alias`: ElasticSearch index alias to push information to. 
* `configuration.elasticsearch_index_mappings`: ElasticSearch mappings json used to characterize the fields that are output from the `pipe_vessels.bigquery_vessel_info_query`.

For example:

```
  "configurations": [
    {
      "bigquery_tracks": "", 
      "bigquery_vessel_info_query": "", 
      "bigquery_vessel_tracks_jinja_query": "", 
      "elasticsearch_index_alias": "", 
      "elasticsearch_index_mappings": "", 
      "name": "", 
      "postgres_table_tracks": "", 
      "target_dataset": ""
    }
  ], 
```

# License

Copyright 2017 Global Fishing Watch

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
