#!/bin/bash
set -e
source pipe-tools-utils

THIS_SCRIPT_DIR="$( cd "$(dirname "${BASH_SOURCE[0]}")" ; pwd -P )"
ASSETS=${THIS_SCRIPT_DIR}/../assets
ARGS=( \
  DATE_RANGE \
  SOURCE \
  TEMP_DATASET \
  TEMP_BUCKET \
  DEST_INSTANCE \
  DEST_CONNECTION_STRING \
  DEST_TABLE \
)

################################################################################
# Validate and extract arguments
################################################################################
display_usage() {
  ARG_NAMES=$(echo "${ARGS[*]}")
  echo -e "\nUsage:\n$0 $ARG_NAMES\n"
}

if [[ $# -ne ${#ARGS[@]} ]]
then
    display_usage
    exit 1
fi

echo "Running $0"
ARG_VALUES=("$@")
PARAMS=()
for index in ${!ARGS[*]}; do
  echo "  ${ARGS[$index]}=${ARG_VALUES[$index]}"
  declare "${ARGS[$index]}"="${ARG_VALUES[$index]}"
done

IFS=, read START_DATE END_DATE <<<"${DATE_RANGE}"
if [[ -z $END_DATE ]]; then
  END_DATE=${START_DATE}
fi
################################################################################
# Extracting the records we need to import to a different temp table
################################################################################
echo "Fetching only the records we need to export"
UUID=$(uuidgen)
TEMP_EXTRACT_TABLE="${TEMP_DATASET}.${UUID//-/_}"
EXTRACT_SQL=${ASSETS}/bigquery/extract-tracks.sql.j2
echo "  Running query to destionation table ${TEMP_EXTRACT_TABLE}"
jinja2 ${EXTRACT_SQL} \
  -D table=${SOURCE//:/.} \
  -D start=${START_DATE} \
  -D end=${END_DATE} \
  | bq --headless query \
  -n 0 \
  --destination_table ${TEMP_EXTRACT_TABLE} \
  --use_legacy_sql=false
if [ "$?" -ne 0 ]; then
  echo "  Unable to fetch the records to export into ${TEMP_EXTRACT_TABLE}"
  exit 1
fi
echo "  Extracted the records to export into ${TEMP_EXTRACT_TABLE}"

################################################################################
# Export records to json files
################################################################################
echo "Exporting records from $TEMP_EXTRACT_TABLE"
TEMP_PATH=gs://${TEMP_BUCKET}/pipe-vessels/publish-postgres-tracks/$( date -u "+%FT%T.%N" )
EXTRACT_PATH=$TEMP_PATH/bq/*.json.gz
bq --headless extract \
  --compression=GZIP \
  --destination_format=NEWLINE_DELIMITED_JSON \
  $TEMP_EXTRACT_TABLE \
  $EXTRACT_PATH
if [ "$?" -ne 0 ]; then
  echo "  Unable to extract ${TEMP_EXTRACT_TABLE} to ${EXTRACT_PATH}"
  exit 1
fi
echo "  Exported records from ${TEMP_EXTRACT_TABLE} to ${EXTRACT_PATH}"

################################################################################
# Delete the temp extract table
################################################################################
echo "Deleting temp extract table ${TEMP_EXTRACT_TABLE}"
bq rm -t -f ${TEMP_EXTRACT_TABLE}
if [ "$?" -ne 0 ]; then
  echo "  Unable to delete the temp extract table ${TEMP_EXTRACT_TABLE}"
  exit 1
fi
echo "  Deleted the temp extract table ${TEMP_EXTRACT_TABLE}"

################################################################################
# Download files locally
################################################################################
echo "Downloading records from ${TEMP_EXTRACT_TABLE} to local disk"
LOCAL_JSON_PATH=./data/json/
mkdir -p ${LOCAL_JSON_PATH}
gsutil -m cp ${EXTRACT_PATH} ${LOCAL_JSON_PATH}
if [ "$?" -ne 0 ]; then
  echo "  Unable to download records data locally from ${TEMP_BUCKET}"
  exit 1
fi
echo "  Downloaded records from ${TEMP_BUCKET}"

################################################################################
# Convert format from json to exported sql format
################################################################################
echo "Converting downloaded files to csv format"
LOCAL_CSV_PATH=./data/csv
LOCAL_CSV_FILE=${LOCAL_CSV_PATH}/data.csv
echo "Creating local csv directory"
mkdir -p ${LOCAL_CSV_PATH}
if [ "$?" -ne 0 ]; then
  echo "  Unable to create local CSV directory"
  exit 1
fi
echo "Converting json records to csv format"
zcat ${LOCAL_JSON_PATH}/*.json.gz | python -m pipe_vessels.postgres.tracks ${LOCAL_CSV_FILE}
if [ "$?" -ne 0 ]; then
  echo "  Unable to convert records from JSON to CSV format"
  exit 1
fi
echo "  Coverted records from JSON to CSV"

################################################################################
# Start the cloudsql proxy
################################################################################
echo "Starting the cloudsql proxy"
cloud_sql_proxy -instances=${DEST_INSTANCE}=tcp:5432 &
echo "  Waiting until database is ready at 127.0.0.1:5432"
sleep 5
i=0
while ! nc -v -w 5 127.0.0.1 5432 < /dev/null; do
  i=`expr $i + 1`
  if [ $i -ge 10 ]; then
    echo "    $(date) - still not reachable, giving up"
    exit 1
  fi
  echo "    $(date) - waiting $i/10"
  sleep 5
done
echo "  Database is ready"

################################################################################
# Load data into postgres
################################################################################
echo "Setting up database for data import"
SETUP_SQL=${ASSETS}/postgres/tracks_setup.sql.j2
jinja2 ${SETUP_SQL} \
  -D table_name=${DEST_TABLE} \
  -D start=${START_DATE} \
  -D end=${END_DATE} \
  | psql -v ON_ERROR_STOP=ON "${DEST_CONNECTION_STRING}"
if [ "$?" -ne 0 ]; then
  echo "  Unable to set database up for data import"
  exit 1
fi

echo "Importing data"
COPY_COMMAND="\copy ${DEST_TABLE} (seg_id, vessel_id, timestamp, position, score, speed, course) from stdin with (format 'csv')"
cat ${LOCAL_CSV_FILE} | psql -v ON_ERROR_STOP=ON "${DEST_CONNECTION_STRING}" -c "$COPY_COMMAND"
if [ "$?" -ne 0 ]; then
  echo "  Unable to import data into postgres"
  exit 1
fi

echo "Indexing data"
INDEX_SQL=${ASSETS}/postgres/tracks_index.sql.j2
jinja2 ${INDEX_SQL} \
  -D table_name=${DEST_TABLE} \
  | psql -v ON_ERROR_STOP=ON "${DEST_CONNECTION_STRING}"
if [ "$?" -ne 0 ]; then
  echo "  Unable to set up indices for imported data"
  exit 1
fi
